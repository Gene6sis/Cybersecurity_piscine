#!/usr/bin/env python3

import requests
from bs4 import BeautifulSoup
import os
import urllib.parse
import argparse

# Supported image extensions
IMAGE_EXTENSIONS = ['.jpg', '.jpeg', '.png', '.gif', '.bmp']

def get_html(url):
    """Get the HTML content of the given URL."""
    try:
        response = requests.get(url)
        response.raise_for_status()
        return response.text
    except requests.exceptions.RequestException as e:
        print(f"Error fetching {url}: {e}")
        return None

def is_image(url):
    """Check if a URL points to an image based on the file extension."""
    parsed_url = urllib.parse.urlparse(url)
    _, ext = os.path.splitext(parsed_url.path)
    return ext.lower() in IMAGE_EXTENSIONS

def save_image(url, path):
    """Download and save an image from the URL to the given path."""
    try:
        response = requests.get(url)
        response.raise_for_status()
        filename = os.path.join(path, os.path.basename(urllib.parse.urlparse(url).path))
        with open(filename, 'wb') as file:
            file.write(response.content)
        print(f"Downloaded {url} to {filename}")
    except requests.exceptions.RequestException as e:
        print(f"Error downloading {url}: {e}")

def extract_images(soup, base_url):
    """Extract all image URLs from the HTML soup."""
    images = []
    for img_tag in soup.find_all("img"):
        img_url = img_tag.get("src")
        if img_url:
            img_url = urllib.parse.urljoin(base_url, img_url)
            if is_image(img_url):
                images.append(img_url)
    return images

def extract_links(soup, base_url):
    """Extract all links from the HTML soup."""
    links = []
    for link_tag in soup.find_all("a"):
        link_url = link_tag.get("href")
        if link_url:
            link_url = urllib.parse.urljoin(base_url, link_url)
            links.append(link_url)
    return links

def spider(url, path, depth, max_depth):
    """Recursively download images from a URL, up to a specified depth."""
    if depth > max_depth:
        return
    html = get_html(url)
    if html is None:
        return
    soup = BeautifulSoup(html, 'html.parser')
    images = extract_images(soup, url)
    for img_url in images:
        save_image(img_url, path)
    
    links = extract_links(soup, url)
    for link in links:
        spider(link, path, depth + 1, max_depth)

def main():
    parser = argparse.ArgumentParser(description="Spider program to download images from a website.", add_help=False)
    parser.add_argument("url", help="The URL to scrape for images.")
    parser.add_argument("-r", action="store_true", help="Recursively download images.")
    parser.add_argument("-l", type=int, default=5, help="Maximum depth for recursive download (default: 5).")
    parser.add_argument("-p", type=str, default="./data/", help="Path to save downloaded images (default: ./data/).")

    args = parser.parse_args()

    # Create the save directory if it doesn't exist
    if not os.path.exists(args.p):
        os.makedirs(args.p)

    # Start spidering with the given parameters
    if args.r:
        spider(args.url, args.p, 0, args.l)
    else:
        html = get_html(args.url)
        if html:
            soup = BeautifulSoup(html, 'html.parser')
            images = extract_images(soup, args.url)
            for img_url in images:
                save_image(img_url, args.p)

if __name__ == "__main__":
    main()
